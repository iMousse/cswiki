import{_ as e,E as p,c as h,m as i,a as s,J as l,w as t,V as n,o as r}from"./chunks/framework.syB9hai_.js";const R=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"src/project/redis/12-Redis原理-数据结构.md","filePath":"src/project/redis/12-Redis原理-数据结构.md","lastUpdated":1730648753000}'),c={name:"src/project/redis/12-Redis原理-数据结构.md"},g=n("",47),o=i("br",null,null,-1),d=i("p",null,"以当前案例来说流程如下：",-1),k=i("li",null,[i("p",null,[i("strong",null,"倒序"),s("依次将数组中的元素拷贝到扩容后的正确位置")])],-1),u=n("",62),m=i("p",null,"1.计算新 hash 表的 realeSize，值取决于当前要做的是扩容还是收缩：",-1),y=i("ul",null,[i("li",null,"如果是扩容，则新 size 为第一个大于等于 dict.ht[0].used + 1 的2^n^")],-1),b=i("ul",null,[i("li",null,"如果是收缩，则新 size 为第一个大于等于 dict.ht[0].used 的 2^n^（不得小于4）")],-1),F=i("p",null,"2.按照新的 realeSize 申请内存空间，创建 dictht，并赋值给 dict.ht[1]",-1),E=i("p",null,"3.设置 dict.rehashidx = 0，标示开始 rehash",-1),z=i("p",null,[i("s",null,"4.将 dict.ht[0] 中的每一个dictEntry 都 rehash 到 dict.ht[1]")],-1),S=n("",219);function C(_,B,L,D,q,v){const a=p("font");return r(),h("div",null,[g,i("p",null,[s("我们向该其中添加一个数字：50,000，这个数字超出了 int16_t 的范围，intset 会自动"),l(a,{color:"red"},{default:t(()=>[s("升级")]),_:1}),s("编码方式到合适的大小。")]),o,d,i("ul",null,[i("li",null,[i("p",null,[s("升级编码为 "),l(a,{color:"red"},{default:t(()=>[s("INTSET_ENC_INT32")]),_:1}),s(", 每个整数占 4字节，并按照新的编码方式及元素个数扩容数组")])]),k]),u,i("p",null,[s("Dict 的 rehash 并不是一次性完成的。试想一下，如果 Dict 中包含数百万的 entry，要在一次 rehash 完成，极有可能导致主线程阻塞。因此 Dict 的 rehash 是分多次、渐进式的完成，因此称为 "),l(a,{color:"red"},{default:t(()=>[s("渐进式rehash")]),_:1}),s("。流程如下：")]),m,y,b,F,E,z,l(a,{color:"red"},{default:t(()=>[s("4.每次执行新增、查询、修改、删除操作时，都检查一下 dict.rehashidx 是否大于 -1，如果是则将 dict.ht[0].table[rehashidx] 的 entry 链表 rehash 到 dict.ht[1]，并且将 rehashidx++。直至 dict.ht[0] 的所有数据都 rehash到 dict.ht[1] ")]),_:1}),S])}const w=e(c,[["render",C]]);export{R as __pageData,w as default};
